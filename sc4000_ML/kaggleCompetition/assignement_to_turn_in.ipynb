{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import xgboost as xgb\n",
    "from packaging import version\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "INPUT = Path(\"/kaggle/input/linking-writing-processes-to-writing-quality\")\n",
    "if not INPUT.exists():\n",
    "    INPUT = Path(\"linking-writing-processes-to-writing-quality\")\n",
    "\n",
    "TRAIN_LOGS = INPUT / \"train_logs.csv\"\n",
    "TRAIN_SCORES = INPUT / \"train_scores.csv\"\n",
    "TEST_LOGS  = INPUT / \"test_logs.csv\"\n",
    "SAMPLE_SUB = INPUT / \"sample_submission.csv\"\n",
    "\n",
    "WORK   = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path(\".\")\n",
    "SUBMIT = WORK / \"submission.csv\"\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 10\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8405898, 11) (2471, 2) (6, 11)\n"
     ]
    }
   ],
   "source": [
    "train_logs = pd.read_csv(TRAIN_LOGS)\n",
    "train_scores = pd.read_csv(TRAIN_SCORES)\n",
    "test_logs = pd.read_csv(TEST_LOGS)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB)\n",
    "\n",
    "for c in [\"down_time\",\"up_time\",\"action_time\",\"event_id\",\"cursor_position\",\"word_count\"]:\n",
    "    for df in (train_logs, test_logs):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(train_logs.shape, train_scores.shape, test_logs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Essay reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              essay\n",
       "0  001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...\n",
       "1  0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...\n",
       "2  0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...\n",
       "3  0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...\n",
       "4  0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EssayBuilder:\n",
    "    MOVE_RX = re.compile(r\"Move From \\[(\\d+), (\\d+)\\] To \\[(\\d+), (\\d+)\\]\")\n",
    "\n",
    "    def reconstruct_all(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        cols = [\"id\",\"event_id\",\"activity\",\"cursor_position\",\"text_change\"]\n",
    "        df = df[cols].sort_values([\"id\",\"event_id\"]).reset_index(drop=True)\n",
    "        out = []\n",
    "        for eid, g in df.groupby(\"id\", sort=False):\n",
    "            out.append({\"id\": eid, \"essay\": self._rebuild_one(g)})\n",
    "        return pd.DataFrame(out)\n",
    "\n",
    "    def _rebuild_one(self, g: pd.DataFrame) -> str:\n",
    "        text = \"\"\n",
    "        for act, cur, chg in g[[\"activity\",\"cursor_position\",\"text_change\"]].itertuples(index=False):\n",
    "            cur = int(cur)\n",
    "            chg = chg if isinstance(chg, str) else \"\"\n",
    "            if act == \"Nonproduction\":\n",
    "                continue\n",
    "            elif act in (\"Input\", \"Paste\"):\n",
    "                L = len(chg)\n",
    "                start = max(cur - L, 0)\n",
    "                text = text[:start] + chg + text[start:]\n",
    "            elif act == \"Remove/Cut\":\n",
    "                L = len(chg)\n",
    "                text = text[:cur] + text[cur + L:]\n",
    "            elif act == \"Replace\" and \"=>\" in chg:\n",
    "                old, new = [p.strip() for p in chg.split(\"=>\", 1)]\n",
    "                start = max(cur - len(new), 0)\n",
    "                end   = start + len(old)\n",
    "                if end < start: end = start\n",
    "                text = text[:start] + new + text[end:]\n",
    "            elif isinstance(act, str) and act.startswith(\"Move From\"):\n",
    "                m = self.MOVE_RX.fullmatch(act.strip())\n",
    "                if not m:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, m.groups())\n",
    "                if x1 == x2:\n",
    "                    continue\n",
    "                if x1 < x2:\n",
    "                    text = text[:x1] + text[y1:y2] + text[x1:y1] + text[y2:]\n",
    "                else:\n",
    "                    text = text[:x2] + text[x1:y1] + text[x2:x1] + text[y1:]\n",
    "        return text\n",
    "\n",
    "builder = EssayBuilder()\n",
    "train_essays = builder.reconstruct_all(train_logs)\n",
    "test_essays  = builder.reconstruct_all(test_logs)\n",
    "\n",
    "train_essays.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (2471, 69) | Test features shape: (3, 69)\n"
     ]
    }
   ],
   "source": [
    "WORD_RX = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "def q_words(text: str):\n",
    "    return WORD_RX.findall(text or \"\")\n",
    "\n",
    "def word_lengths(text: str):\n",
    "    return [len(w) for w in q_words(text)]\n",
    "\n",
    "def paragraph_word_lengths(text: str):\n",
    "    paras = [p for p in (text or \"\").split(\"\\n\")]\n",
    "    pls = []\n",
    "    for p in paras:\n",
    "        if p.strip() == \"\":\n",
    "            continue\n",
    "        pls.append(len(WORD_RX.findall(p)))\n",
    "    return pls\n",
    "\n",
    "def count_chars_typed_from_logs(g: pd.DataFrame) -> int:\n",
    "    total = 0\n",
    "    for act, chg in g[[\"activity\",\"text_change\"]].itertuples(index=False):\n",
    "        if not isinstance(chg, str):\n",
    "            continue\n",
    "        if act in (\"Input\", \"Paste\"):\n",
    "            total += len(chg)\n",
    "        elif act == \"Replace\" and \"=>\" in chg:\n",
    "            old, new = [p.strip() for p in chg.split(\"=>\", 1)]\n",
    "            total += len(new)\n",
    "    return total\n",
    "\n",
    "def median_iki_with_lag(g: pd.DataFrame, lag: int) -> float:\n",
    "    sub = g[g[\"activity\"].isin([\"Input\",\"Remove/Cut\"])].copy()\n",
    "    sub[\"up_time_lag\"] = sub.groupby(\"id\")[\"up_time\"].shift(lag)\n",
    "    val = (sub[\"down_time\"] - sub[\"up_time_lag\"]).dropna()\n",
    "    val = val[val >= 0]\n",
    "    return float(val.median()) if len(val) else 0.0\n",
    "\n",
    "def input_bursts_count(g: pd.DataFrame) -> int:\n",
    "    sub = g[g[\"activity\"].isin([\"Input\",\"Remove/Cut\"])].copy()\n",
    "    sub[\"up_time_lag\"] = sub.groupby(\"id\")[\"up_time\"].shift(1)\n",
    "    sub[\"gap\"] = (sub[\"down_time\"] - sub[\"up_time_lag\"])\n",
    "    sub = sub.dropna()\n",
    "    sub = sub[sub[\"gap\"] >= 0]\n",
    "    is_fast = (sub[\"gap\"] < 2000).astype(int).values\n",
    "    if len(is_fast) == 0:\n",
    "        return 0\n",
    "    runs = 0\n",
    "    prev = 0\n",
    "    for z in is_fast:\n",
    "        if z == 1 and prev == 0:\n",
    "            runs += 1\n",
    "        prev = z\n",
    "    return int(runs)\n",
    "\n",
    "def punctuation_counts(text: str):\n",
    "    t = text or \"\"\n",
    "    return dict(\n",
    "        n_dot=t.count(\".\"),\n",
    "        n_comma=t.count(\",\"),\n",
    "        n_hyphen=t.count(\"-\"),\n",
    "        n_space=t.count(\" \"),\n",
    "        n_line_breaks=t.count(\"\\n\"),\n",
    "        n_spec_char=sum(1 for ch in t if (not ch.isalnum()) and (not ch.isspace()))\n",
    "    )\n",
    "\n",
    "def typo_like_counts(text: str):\n",
    "    t = text or \"\"\n",
    "    return dict(n_typos_dot_comma = len(re.findall(r\"\\s[.,]\", t)))\n",
    "\n",
    "def build_features(df_logs: pd.DataFrame, df_essays: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_logs.sort_values([\"id\",\"event_id\"]).copy()\n",
    "    df[\"next_down_time\"] = df.groupby(\"id\")[\"down_time\"].shift(-1)\n",
    "    df[\"gap_ms\"] = df[\"next_down_time\"] - df[\"up_time\"]\n",
    "    df[\"gap_ms\"] = df[\"gap_ms\"].where(df[\"gap_ms\"] >= 0)\n",
    "\n",
    "    def is_backspace(s):\n",
    "        s = str(s).lower()\n",
    "        return (\"backspace\" in s) or (\"back space\" in s) or (s == \"back\")\n",
    "    def is_shift(s):\n",
    "        return str(s).lower() == \"shift\"\n",
    "    def is_space(s):\n",
    "        return str(s).lower() == \"space\"\n",
    "\n",
    "    df[\"is_backspace\"] = df[\"down_event\"].map(is_backspace) | df[\"up_event\"].map(is_backspace)\n",
    "    df[\"is_shift\"]     = df[\"down_event\"].map(is_shift)     | df[\"up_event\"].map(is_shift)\n",
    "    df[\"is_space\"]     = df[\"down_event\"].map(is_space)     | df[\"up_event\"].map(is_space)\n",
    "\n",
    "    rows = []\n",
    "    essay_map = dict(zip(df_essays[\"id\"].values, df_essays[\"essay\"].values))\n",
    "\n",
    "    for eid, g in df.groupby(\"id\"):\n",
    "        t0, t1 = g[\"down_time\"].min(), g[\"up_time\"].max()\n",
    "        event_count = int(len(g))\n",
    "        vc = g[\"activity\"].astype(str).value_counts()\n",
    "        c_input   = int(vc.get(\"Input\", 0))\n",
    "        c_remove  = int(vc.get(\"Remove/Cut\", 0))\n",
    "        c_paste   = int(vc.get(\"Paste\", 0))\n",
    "        c_replace = int(vc.get(\"Replace\", 0))\n",
    "        c_move    = sum(int(v) for k, v in vc.items() if isinstance(k, str) and k.startswith(\"Move From\"))\n",
    "        edit_cnt  = c_remove + c_paste + c_replace + c_move\n",
    "\n",
    "        p_input  = (c_input / event_count) if event_count else 0.0\n",
    "        p_shift  = float(g[\"is_shift\"].mean()) if event_count else 0.0\n",
    "        p_space  = float(g[\"is_space\"].mean()) if event_count else 0.0\n",
    "\n",
    "        gaps = g[\"gap_ms\"].dropna()\n",
    "        gaps = gaps[gaps >= 0]\n",
    "        med_pause = float(gaps.median()) if len(gaps) else 0.0\n",
    "        n_pause_3s = int((gaps > 3000).sum())\n",
    "\n",
    "        med_IKI      = median_iki_with_lag(g, 1)\n",
    "        med_IKI_lag2 = median_iki_with_lag(g, 2)\n",
    "        med_IKI_lag3 = median_iki_with_lag(g, 3)\n",
    "        med_IKI_lag4 = median_iki_with_lag(g, 4)\n",
    "\n",
    "        s_action_time = float(g[\"action_time\"].std()) if g[\"action_time\"].notna().sum() else 0.0\n",
    "        total_typed_chars = count_chars_typed_from_logs(g)\n",
    "\n",
    "        essay_text = essay_map.get(eid, \"\")\n",
    "        prod_total_char = len(essay_text)\n",
    "        punct = punctuation_counts(essay_text)\n",
    "        typo_like = typo_like_counts(essay_text)\n",
    "\n",
    "        wl = word_lengths(essay_text)\n",
    "        sd_length_word = float(np.std(wl)) if wl else 0.0\n",
    "\n",
    "        def count_len(k): \n",
    "            return int(np.sum(np.array(wl) == k)) if wl else 0\n",
    "        n_q2  = count_len(2)\n",
    "        n_q6  = count_len(6)\n",
    "        n_q7  = count_len(7)\n",
    "        n_q8  = count_len(8)\n",
    "        n_q10 = count_len(10)\n",
    "\n",
    "        rng = [w for w in wl if 8 <= w <= 12]\n",
    "        seq_max_min_8_12 = (max(rng) - min(rng)) if rng else 0\n",
    "\n",
    "        pls = paragraph_word_lengths(essay_text)\n",
    "        mean_paragraph = float(np.mean(pls)) if pls else 0.0\n",
    "\n",
    "        total_words = max(1, len(wl))\n",
    "        p_dot   = punct[\"n_dot\"]   / total_words\n",
    "        p_comma = punct[\"n_comma\"] / total_words\n",
    "\n",
    "        n_backspace = int(g[\"is_backspace\"].sum())\n",
    "        n_shift     = int(g[\"is_shift\"].sum())\n",
    "        burst_input = input_bursts_count(g)\n",
    "\n",
    "        t20 = (t0 + 20*60*1000) if pd.notnull(t0) else None\n",
    "        if t20 is not None and \"word_count\" in g.columns and g[\"word_count\"].notna().any():\n",
    "            before_20 = g.loc[g[\"down_time\"] <= t20, \"word_count\"].max() if (g[\"down_time\"] <= t20).any() else 0\n",
    "            max_wc = int(g[\"word_count\"].max()) if g[\"word_count\"].notna().any() else len(wl)\n",
    "            n_words_after_20m = int(max_wc - (before_20 if pd.notnull(before_20) else 0))\n",
    "        else:\n",
    "            n_words_after_20m = 0\n",
    "\n",
    "        if t0 is not None and pd.notnull(t0):\n",
    "            t30 = t0 + 30*60*1000\n",
    "            if \"word_count\" in g.columns and g[\"word_count\"].notna().any():\n",
    "                wc_0 = g.loc[g[\"down_time\"] >= t0, \"word_count\"].min()\n",
    "                wc_30 = g.loc[g[\"up_time\"] <= t30, \"word_count\"].max() if (g[\"up_time\"] <= t30).any() else wc_0\n",
    "                dt = 30.0\n",
    "                mean_input_30m = float((wc_30 - wc_0) / max(dt, 1e-9)) if wc_0 is not None and wc_30 is not None else 0.0\n",
    "            else:\n",
    "                mean_input_30m = 0.0\n",
    "        else:\n",
    "            mean_input_30m = 0.0\n",
    "\n",
    "        prod_lbreak_shift = punct[\"n_line_breaks\"] * (p_shift if not np.isnan(p_shift) else 0.0)\n",
    "\n",
    "        # production curve deciles (time-normalized)\n",
    "        if pd.notnull(t0) and pd.notnull(t1) and t1 > t0:\n",
    "            T = float(t1 - t0)\n",
    "            g2 = g.copy()\n",
    "            g2[\"t_norm\"] = (g2[\"down_time\"] - t0) / T\n",
    "            if \"word_count\" in g2.columns and g2[\"word_count\"].notna().any():\n",
    "                series = g2[[\"t_norm\",\"word_count\"]].dropna().sort_values(\"t_norm\")\n",
    "                wc_times = series[\"t_norm\"].values\n",
    "                wc_vals  = series[\"word_count\"].values\n",
    "            else:\n",
    "                g2[\"ins_len\"] = 0\n",
    "                m_in = g2[\"activity\"].isin([\"Input\",\"Paste\"])\n",
    "                g2.loc[m_in, \"ins_len\"] = g2.loc[m_in, \"text_change\"].fillna(\"\").map(len)\n",
    "                series = g2[[\"t_norm\",\"ins_len\"]].sort_values(\"t_norm\")\n",
    "                wc_times = series[\"t_norm\"].values\n",
    "                wc_vals  = np.cumsum(series[\"ins_len\"].values)\n",
    "            deciles = np.linspace(0.1, 1.0, 10)\n",
    "            prod_share = []\n",
    "            if len(wc_vals) >= 2 and wc_vals[-1] > 0:\n",
    "                total = float(wc_vals[-1])\n",
    "                for d in deciles:\n",
    "                    idx = np.searchsorted(wc_times, d, side=\"right\") - 1\n",
    "                    idx = np.clip(idx, 0, len(wc_vals) - 1)\n",
    "                    prod_share.append(float(wc_vals[idx] / total))\n",
    "            else:\n",
    "                prod_share = [0.0] * 10\n",
    "            early_20 = prod_share[1]\n",
    "            mid_50   = prod_share[4]\n",
    "            late_80  = prod_share[7]\n",
    "            early_late_ratio = (early_20 + 1e-9) / (1.0 - late_80 + 1e-9)\n",
    "        else:\n",
    "            prod_share = [0.0] * 10\n",
    "            early_20 = mid_50 = late_80 = early_late_ratio = 0.0\n",
    "\n",
    "        # pause distribution shape\n",
    "        p50 = float(gaps.quantile(0.50)) if len(gaps) else 0.0\n",
    "        p75 = float(gaps.quantile(0.75)) if len(gaps) else 0.0\n",
    "        p90 = float(gaps.quantile(0.90)) if len(gaps) else 0.0\n",
    "        p95 = float(gaps.quantile(0.95)) if len(gaps) else 0.0\n",
    "        p99 = float(gaps.quantile(0.99)) if len(gaps) else 0.0\n",
    "\n",
    "        # edit intensity + timing & backspace streaks\n",
    "        edit_total = float(edit_cnt)\n",
    "        edit_per_100_words = (edit_total / max(1, len(wl))) * 100.0\n",
    "        backspace_per_100_words = (n_backspace / max(1, len(wl))) * 100.0\n",
    "        if pd.notnull(t0) and pd.notnull(t1) and t1 > t0:\n",
    "            cutoff = t0 + 0.8 * (t1 - t0)\n",
    "            late_edits = g[(g[\"down_time\"] >= cutoff) & g[\"activity\"].isin([\"Remove/Cut\",\"Replace\",\"Paste\"])].shape[0]\n",
    "            frac_late_edits = float(late_edits / max(1, edit_cnt))\n",
    "        else:\n",
    "            frac_late_edits = 0.0\n",
    "\n",
    "        m_bs = g[\"is_backspace\"].values.astype(int)\n",
    "        streaks, cur = [], 0\n",
    "        for z in m_bs:\n",
    "            if z: cur += 1\n",
    "            elif cur > 0:\n",
    "                streaks.append(cur); cur = 0\n",
    "        if cur > 0: streaks.append(cur)\n",
    "        bs_streak_max = int(max(streaks)) if streaks else 0\n",
    "        bs_streak_mean = float(np.mean(streaks)) if streaks else 0.0\n",
    "\n",
    "        act_probs = g[\"activity\"].astype(str).value_counts(normalize=True).values\n",
    "        act_entropy = float(-(act_probs * np.log(act_probs + 1e-12)).sum())\n",
    "\n",
    "        session_min = ((t1 - t0) / 60000.0) if pd.notnull(t0) and pd.notnull(t1) and t1 > t0 else 0.0\n",
    "        events_per_min = (event_count / max(session_min, 1e-9)) if session_min > 0 else 0.0\n",
    "\n",
    "        move_ratio    = c_move    / max(1, event_count)\n",
    "        replace_ratio = c_replace / max(1, event_count)\n",
    "        paste_ratio   = c_paste   / max(1, event_count)\n",
    "        edit_share    = edit_cnt  / max(1, event_count)\n",
    "\n",
    "        sub_ir = g[g[\"activity\"].isin([\"Input\",\"Remove/Cut\"])].copy()\n",
    "        sub_ir[\"up_time_lag\"] = sub_ir[\"up_time\"].shift(1)\n",
    "        iki = (sub_ir[\"down_time\"] - sub_ir[\"up_time_lag\"]).dropna()\n",
    "        iki = iki[iki >= 0]\n",
    "        iki_std = float(np.std(iki)) if len(iki) else 0.0\n",
    "        iki_cv  = float(iki_std / (np.median(iki)+1e-9)) if len(iki) else 0.0\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": eid,\n",
    "            \"prod_total_char\": prod_total_char,\n",
    "            \"_total_char\": total_typed_chars,\n",
    "            **punct, **typo_like,\n",
    "            \"p_dot\": p_dot, \"p_comma\": p_comma, \"p_space\": p_space,\n",
    "            \"sd_length_word\": sd_length_word,\n",
    "            \"n_q2\": n_q2, \"n_q6\": n_q6, \"n_q7\": n_q7, \"n_q8\": n_q8, \"n_q10\": n_q10,\n",
    "            \"seq_max_min_8_12\": seq_max_min_8_12,\n",
    "            \"mean_paragraph\": mean_paragraph,\n",
    "            \"med_IKI\": med_IKI, \"med_IKI_lag2\": med_IKI_lag2, \"med_IKI_lag3\": med_IKI_lag3, \"med_IKI_lag4\": med_IKI_lag4,\n",
    "            \"med_pause\": med_pause, \"n_pause_3s\": n_pause_3s, \"s_action_time\": s_action_time,\n",
    "            \"burst_input\": burst_input, \"n_backspace\": n_backspace, \"n_shift\": n_shift,\n",
    "            \"p_shift\": p_shift, \"p_input\": p_input,\n",
    "            \"n_words_after_20m\": n_words_after_20m, \"mean_input_30m\": mean_input_30m,\n",
    "            \"prod_lbreak_shift\": prod_lbreak_shift,\n",
    "            **{f\"prod_d{int((i+1)*10)}\": prod_share[i] for i in range(10)},\n",
    "            \"prod_early20\": early_20, \"prod_mid50\": mid_50, \"prod_late80\": late_80, \"early_late_ratio\": early_late_ratio,\n",
    "            \"pause_p50\": p50, \"pause_p75\": p75, \"pause_p90\": p90, \"pause_p95\": p95, \"pause_p99\": p99,\n",
    "            \"edit_per_100_words\": edit_per_100_words, \"backspace_per_100_words\": backspace_per_100_words,\n",
    "            \"frac_late_edits\": frac_late_edits,\n",
    "            \"bs_streak_max\": bs_streak_max, \"bs_streak_mean\": bs_streak_mean,\n",
    "            \"act_entropy\": act_entropy,\n",
    "            \"session_min\": session_min, \"events_per_min\": events_per_min,\n",
    "            \"move_ratio\": move_ratio, \"replace_ratio\": replace_ratio, \"paste_ratio\": paste_ratio, \"edit_share\": edit_share,\n",
    "            \"iki_std\": iki_std, \"iki_cv\": iki_cv,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_feats = build_features(train_logs, train_essays)\n",
    "test_feats  = build_features(test_logs,  test_essays)\n",
    "\n",
    "df_train = train_feats.merge(train_scores, on=\"id\", how=\"left\")\n",
    "feature_cols = [c for c in train_feats.columns if c != \"id\"]\n",
    "print(\"Train features shape:\", train_feats.shape, \"| Test features shape:\", test_feats.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[feature_cols].fillna(0.0)\n",
    "y = df_train[\"score\"].astype(float).values\n",
    "X_test = test_feats[feature_cols].fillna(0.0)\n",
    "\n",
    "TREE_METHOD = \"gpu_hist\" if os.environ.get(\"CUDA_VISIBLE_DEVICES\") not in (None, \"\") else \"hist\"\n",
    "USE_GPU_CAT = (os.environ.get(\"CUDA_VISIBLE_DEVICES\") not in (None, \"\"))\n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# XGB\n",
    "xgb_best_params = {\n",
    "    'n_estimators': 3136,\n",
    "    'learning_rate': 0.010121181166477724,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 5.111879961449728,\n",
    "    'gamma': 0.8033271268511519,\n",
    "    'subsample': 0.6752734231760704,\n",
    "    'colsample_bytree': 0.5573188522513011,\n",
    "    'reg_alpha': 0.8909929789441696,\n",
    "    'reg_lambda': 0.9941415870261027,\n",
    "    'max_bin': 452,\n",
    "    'grow_policy': 'depthwise',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': TREE_METHOD,\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# CatBoost\n",
    "cat_best_params = {\n",
    "    'iterations': 5915,\n",
    "    'learning_rate': 0.015600644582174339,\n",
    "    'depth': 5,\n",
    "    'l2_leaf_reg': 1.7350901516859896,\n",
    "    'random_strength': 0.07440670993932308,\n",
    "    'bagging_temperature': 1.4205571202737526,\n",
    "    'subsample': 0.5653366003208794,\n",
    "    'leaf_estimation_iterations': 8,\n",
    "    'loss_function': 'RMSE',\n",
    "}\n",
    "\n",
    "# LightGBM\n",
    "lgb_best_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03554388601967273,\n",
    "    'num_leaves': 105,\n",
    "    'max_depth': 4,\n",
    "    'lambda_l1': 2.0306366179907154,\n",
    "    'lambda_l2': 3.502509429630488,\n",
    "    'min_data_in_leaf': 18,\n",
    "    'min_sum_hessian_in_leaf': 0.003416106525862324,\n",
    "    'min_gain_to_split': 0.39791230620879725,\n",
    "    'feature_fraction': 0.6164560326658721,\n",
    "    'bagging_fraction': 0.9835406089972294,\n",
    "    'bagging_freq': 2,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "}\n",
    "lgb_num_boost_round = 6815\n",
    "\n",
    "# Extra Trees\n",
    "extra_trees_best_params = {\n",
    "  'n_estimators': 1160, \n",
    "  'max_depth': 40, \n",
    "  'min_samples_split': 8, \n",
    "  'min_samples_leaf': 1, \n",
    "  'max_features': 0.49666738375873354, \n",
    "  'bootstrap': False\n",
    "}\n",
    "\n",
    "# kNN\n",
    "knn_best_params = {\n",
    "  'n_neighbors': 15,\n",
    "  'weights': 'distance', \n",
    "  'p': 1, \n",
    "  'leaf_size': 18\n",
    "}\n",
    "\n",
    "# Ridge\n",
    "ridge_best_params = {\n",
    "  'alpha': 99.99512488296149\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_np(a, b):\n",
    "    return float(np.mean((a - b) ** 2)) ** 0.5\n",
    "\n",
    "def best_iter_predict_xgb(model, X_):\n",
    "    if hasattr(model, \"best_iteration\") and model.best_iteration is not None:\n",
    "        try:\n",
    "            return model.predict(X_, iteration_range=(0, model.best_iteration))\n",
    "        except TypeError:\n",
    "            return model.predict(X_, ntree_limit=getattr(model, \"best_ntree_limit\", 0))\n",
    "    return model.predict(X_)\n",
    "\n",
    "def train_xgb_with_es(params, xtr, ytr, xva, yva):\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    if version.parse(xgb.__version__) >= version.parse(\"2.0.0\"):\n",
    "        callbacks = [xgb.callback.EarlyStopping(rounds=300, save_best=True, data_name=\"validation_0\", metric_name=\"rmse\")]\n",
    "        model.fit(xtr, ytr, eval_set=[(xva, yva)], eval_metric=\"rmse\", verbose=False, callbacks=callbacks)\n",
    "    else:\n",
    "        model.fit(xtr, ytr, eval_set=[(xva, yva)], eval_metric=\"rmse\", verbose=False, early_stopping_rounds=300)\n",
    "    return model\n",
    "\n",
    "def train_cat_with_es(params, xtr, ytr, xva, yva, use_gpu):\n",
    "    model = CatBoostRegressor(\n",
    "        **params,\n",
    "        task_type=(\"GPU\" if use_gpu else \"CPU\"),\n",
    "        random_state=SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "    train_pool = Pool(xtr, ytr)\n",
    "    valid_pool = Pool(xva, yva)\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=300, use_best_model=True, verbose=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5-fold CV (XGB + CAT + LGB)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 | XGB 0.57919 | CAT 0.57404 | LGB 0.57941 | ET 0.59355 | RIDGE 0.61927 | KNN 0.63610\n",
      "Fold 1 | XGB 0.51020 | CAT 0.51273 | LGB 0.50603 | ET 0.51546 | RIDGE 0.55003 | KNN 0.58027\n",
      "Fold 2 | XGB 0.65643 | CAT 0.65754 | LGB 0.65543 | ET 0.66359 | RIDGE 0.74116 | KNN 0.70498\n",
      "Fold 3 | XGB 0.60569 | CAT 0.60776 | LGB 0.61446 | ET 0.64173 | RIDGE 0.67011 | KNN 0.73048\n",
      "Fold 4 | XGB 0.58983 | CAT 0.58413 | LGB 0.59611 | ET 0.59606 | RIDGE 0.61831 | KNN 0.66668\n",
      "Fold 5 | XGB 0.60040 | CAT 0.60450 | LGB 0.61438 | ET 0.60569 | RIDGE 0.63963 | KNN 0.69435\n",
      "Fold 6 | XGB 0.61656 | CAT 0.61968 | LGB 0.63311 | ET 0.65492 | RIDGE 0.69295 | KNN 0.71438\n",
      "Fold 7 | XGB 0.62079 | CAT 0.61664 | LGB 0.62525 | ET 0.63684 | RIDGE 0.65332 | KNN 0.67658\n",
      "Fold 8 | XGB 0.60543 | CAT 0.60394 | LGB 0.61701 | ET 0.64228 | RIDGE 0.66207 | KNN 0.71348\n",
      "Fold 9 | XGB 0.58544 | CAT 0.58593 | LGB 0.58406 | ET 0.58405 | RIDGE 0.65741 | KNN 0.60914\n",
      "OOF RMSE — XGB: 0.59805 | CAT: 0.59774 | LGB: 0.60376 | ET: 0.61484 | RIDGE: 0.65216 | KNN: 0.67428\n"
     ]
    }
   ],
   "source": [
    "# OOF holders\n",
    "oof_xgb = np.zeros(len(X)); oof_cat = np.zeros(len(X)); oof_lgb = np.zeros(len(X))\n",
    "oof_et  = np.zeros(len(X)); oof_rg  = np.zeros(len(X)); oof_knn = np.zeros(len(X))\n",
    "\n",
    "# Test fold predictions\n",
    "test_preds_xgb_folds, test_preds_cat_folds, test_preds_lgb_folds = [], [], []\n",
    "test_preds_et_folds,  test_preds_rg_folds,  test_preds_knn_folds  = [], [], []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    xtr, xva = X.iloc[trn_idx], X.iloc[val_idx]           # F2\n",
    "    ytr, yva = y[trn_idx], y[val_idx]\n",
    "\n",
    "    # XGBoost (F2)\n",
    "    xgb_model = train_xgb_with_es(xgb_best_params, xtr, ytr, xva, yva)\n",
    "    pv_xgb = best_iter_predict_xgb(xgb_model, xva)\n",
    "    oof_xgb[val_idx] = pv_xgb\n",
    "    test_preds_xgb_folds.append(best_iter_predict_xgb(xgb_model, X_test))\n",
    "\n",
    "    # CatBoost (F2)\n",
    "    cat_model = train_cat_with_es(cat_best_params, xtr, ytr, xva, yva, USE_GPU_CAT)\n",
    "    pv_cat = cat_model.predict(xva)\n",
    "    oof_cat[val_idx] = pv_cat\n",
    "    test_preds_cat_folds.append(cat_model.predict(X_test))\n",
    "\n",
    "    # LightGBM (F2; train API)\n",
    "    dtr = lgb.Dataset(xtr, ytr); dva = lgb.Dataset(xva, yva, reference=dtr)\n",
    "    lgbm = lgb.train(\n",
    "        lgb_best_params,\n",
    "        dtr,\n",
    "        num_boost_round=lgb_num_boost_round,\n",
    "        valid_sets=[dva],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False),\n",
    "                   lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "    pv_lgb = lgbm.predict(xva, num_iteration=lgbm.best_iteration)\n",
    "    oof_lgb[val_idx] = pv_lgb\n",
    "    test_preds_lgb_folds.append(lgbm.predict(X_test, num_iteration=lgbm.best_iteration))\n",
    "\n",
    "    # ExtraTrees (F2)\n",
    "    et_model = ExtraTreesRegressor(\n",
    "        **extra_trees_best_params,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    et_model.fit(xtr, ytr)\n",
    "    pv_et = et_model.predict(xva)\n",
    "    oof_et[val_idx] = pv_et\n",
    "    test_preds_et_folds.append(et_model.predict(X_test))\n",
    "\n",
    "    # KNN (F2) with scaling\n",
    "    knn_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsRegressor(**knn_best_params))\n",
    "    ])\n",
    "    knn_pipe.fit(xtr, ytr)\n",
    "    pv_knn = knn_pipe.predict(xva)\n",
    "    oof_knn[val_idx] = pv_knn\n",
    "    test_preds_knn_folds.append(knn_pipe.predict(X_test))\n",
    "\n",
    "    # Ridge (F2) with scaling\n",
    "    ridge_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(**ridge_best_params, random_state=SEED))\n",
    "    ])\n",
    "    ridge_pipe.fit(xtr, ytr)\n",
    "    pv_rg = ridge_pipe.predict(xva)\n",
    "    oof_rg[val_idx] = pv_rg\n",
    "    test_preds_rg_folds.append(ridge_pipe.predict(X_test))\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold} | \"\n",
    "        f\"XGB {rmse_np(yva, pv_xgb):.5f} | \"\n",
    "        f\"CAT {rmse_np(yva, pv_cat):.5f} | \"\n",
    "        f\"LGB {rmse_np(yva, pv_lgb):.5f} | \"\n",
    "        f\"ET {rmse_np(yva, pv_et):.5f} | \"\n",
    "        f\"RIDGE {rmse_np(yva, pv_rg):.5f} | \"\n",
    "        f\"KNN {rmse_np(yva, pv_knn):.5f}\"\n",
    "    )\n",
    "\n",
    "# Single-model OOF scores\n",
    "rmse_xgb = rmse_np(y, oof_xgb)\n",
    "rmse_cat = rmse_np(y, oof_cat)\n",
    "rmse_lgb = rmse_np(y, oof_lgb)\n",
    "rmse_et  = rmse_np(y, oof_et)\n",
    "rmse_rg  = rmse_np(y, oof_rg)\n",
    "rmse_knn = rmse_np(y, oof_knn)\n",
    "print(\n",
    "    f\"OOF RMSE — XGB: {rmse_xgb:.5f} | CAT: {rmse_cat:.5f} | LGB: {rmse_lgb:.5f} | \"\n",
    "    f\"ET: {rmse_et:.5f} | RIDGE: {rmse_rg:.5f} | KNN: {rmse_knn:.5f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ridge stack & submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF RMSE — Ridge blend (XGB+CAT+LGB+ET+RIDGE+KNN): 0.59473\n",
      "Blend coefficients: [ 0.55891823  0.50401011 -0.07178017 -0.00623127  0.11177279 -0.08529851] intercept: -0.02718735481870338\n",
      "Saved submission to: submission.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ridge stacking over 6 models\n",
    "Z = np.column_stack([oof_xgb, oof_cat, oof_lgb, oof_et, oof_rg, oof_knn])\n",
    "meta = Ridge(alpha=0.01, fit_intercept=True, random_state=SEED)\n",
    "meta.fit(Z, y)\n",
    "oof_blend = meta.predict(Z)\n",
    "rmse_blend = rmse_np(y, oof_blend)\n",
    "print(f\"OOF RMSE — Ridge blend (XGB+CAT+LGB+ET+RIDGE+KNN): {rmse_blend:.5f}\")\n",
    "print(\"Blend coefficients:\", meta.coef_, \"intercept:\", meta.intercept_)\n",
    "\n",
    "\n",
    "# Build test predictions\n",
    "pred_test_xgb = np.mean(np.column_stack(test_preds_xgb_folds), axis=1)\n",
    "pred_test_cat = np.mean(np.column_stack(test_preds_cat_folds), axis=1)\n",
    "pred_test_lgb = np.mean(np.column_stack(test_preds_lgb_folds), axis=1)\n",
    "pred_test_et  = np.mean(np.column_stack(test_preds_et_folds),  axis=1)\n",
    "pred_test_rg  = np.mean(np.column_stack(test_preds_rg_folds),  axis=1)\n",
    "pred_test_knn = np.mean(np.column_stack(test_preds_knn_folds), axis=1)\n",
    "\n",
    "Z_test = np.column_stack([pred_test_xgb, pred_test_cat, pred_test_lgb,\n",
    "                          pred_test_et, pred_test_rg, pred_test_knn])\n",
    "pred_test = np.clip(meta.predict(Z_test), 0.0, 6.0)\n",
    "\n",
    "# Save submission + summary\n",
    "sub = sample_sub.copy()\n",
    "sub = sub[[\"id\"]].merge(pd.DataFrame({\"id\": test_feats[\"id\"], \"score\": pred_test}), on=\"id\", how=\"left\")\n",
    "sub.to_csv(SUBMIT, index=False)\n",
    "\n",
    "\n",
    "print(\"Saved submission to:\", SUBMIT)\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
